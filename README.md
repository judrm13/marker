# Automarking

In this folder are a bunch of scripts to help with automarking assignments, specifically with Quercus.

**Note**: Windows is not supported. This *should* work on Linux as well, but I'm primarily using macOS. Some of the scripts may not work on `sh`, so make sure you're running them with `bash`.

---

## General layout of things

All of the scripts here are designed to work with the following directory structure for each of the assignments / exercises:
```
assignment_dir/
├── .metadata
├── marksheet.csv  // Can rename 
├── testsuite.csv  // Can rename
├── candidates/
│   ├── utorid1/
│   │   └── soln.c
│   ├── utorid2/
│   │   └── soln.c
│   ├── utorid3/
│   │    └── soln.c
│   └── ...
└── extra-files/
    ├── Makefile
    ├── testing_files/
    │   ├── test1.c
    │   └── ...
    ├── other_required_file.c
    └── ...
```
- `.metadata` contains the course id and assignment id from quercus. For instance, the format is:
    ```
    160057:343885
    ```
- `extra-files` holds the common files for compiling and testing the code.
- `candidates` contains one directory per student (named with their utorid). Each of these directories initially contains their submitted code for the assignment.
- `testsuite.csv` contains the information on the test cases. Each row consists of, in order:
    - Marks for the test case (only integers)
    - Maximum time allowed for the test case (in seconds)
    - Shell command to run the test case. This can involve either running a custom script or running a C executable. The return value of this command is used to judge whether or not the test passed. Don't try using fancy one-liners here. Put them in a shell script and call the script in this command to ensure compatibility.
    - Description of the test case (This is to put in the report for the students.)
- `marksheet.csv` is a file that contains 2 columns. The first is utorid, and the second the total mark of the student. At the start, the mark for all students is blank. When the automarker script is run, it looks at this file, and only tests the students who do not have a mark yet.
    - Why only the unmarked ones? There's always a bunch of remarks, and instead of copying over all of the marking code into separate directories to remark individual assignments, we can simply just remove the mark from beside the utorid and re-run the automarker. It won't remark the others.

Once we have this structure, everything from `extra-files` can be copied into the individual submission directories, and we can compile it however we need by defining the appropriate `Makefile`, and running the tests.

Ideally, you should have a class list (from Quercus > Grades > Export) saved. We will refer to this as `quercus.csv`.

---

### Workflow example

- You will need to get an Access token for Quercus. Find it at (Quercus > Account > Settings > Generate New Token).

- **`download_submissions.py`**: Downloads all the files for a given assignment from quercus and sets up the initial directory structure. It also creates the `.metadata` file in the directory which all the other scripts expect.

- (Manually) add to the `extra-files` directory all the required test files, and a `Makefile` that compiles the testing programs with `make all`. Also, add the `testsuite.csv` file to the root of the assignment directory. 
    - This step can be skipped if you use a source directory with prepare.sh (more below)

- **`prepare.sh`**: Takes everything from the `extra-files` directory and puts it into the individual submission directories. Uses the `Makefile` to compile the code and saves any warnings in `compile.log` inside the submission directories. It also creates the default `marksheet.csv` file with blank marks for everyone.
    - This script will optionally take in a source directory as an argument, and will import `testsuite.csv` and `Makefile` from this folder and place them in the right places.
    - Additionally, if the source directory has a file called `.extra_files`, all file / folder names listed in this will automatically
    be imported into the `extra=files/` directory. (typically you would not put `Makefile` and `testsuite.csv` in here. They are handled by default).

- **`automark.sh`**: Checks the `marksheet.csv` file, and runs the required tests based on `testsuite.csv`. Tests are only run for students with no mark recorded in the marksheet. Reports are generated in `report.txt` inside the student's folder.
    - The script accepts optional arguments to change the expected names for all the 3 files above, if needed.

- **`upload_reports.py`**: Uploads the report files to Quercus as a comment on the individual subissions.

- **`upload_marks.py`**: Uploads all the marks to Quercus given the marksheet csv file generated by the automarker.

---

### Example

For example, to mark ex1, you would run the following commands (while in this directory):
```bash
# Downloads files for ex1 into ../tests/ex1, calls 
# the student files ex1.c
python3 download_submissions.py 160057 346613 ../tests/ex1 ex1.c

# Prepares the submissions using the config in the 
# ORIGINAL exercise directory
./prepare.sh -s ../ex/1/ ../tests/ex1

# Run the actual automarker. Can optionally add in 
# (-r prerun.txt) to rename log file generated
./automark.sh ../tests/ex1
```

Then, one you have confirmed the marks look alright, and the reports were generated correctly, run the following to upload the marks and reports:
```bash
python3 upload_marks.py ../tests/ex1/marksheet.csv

# Change the name of the report here if you had the
# `-r <reportname>` flag set for the automarker
python3 upload_reports ../tests/ex1 report.txt
```

---

### Templates

The `templates` folder in this directory contains templates for the following:
- A basic C file allowing you to define your own tests and run one or all of them.
- Accompanying `testsuite.csv` for the template testing file to run the tests.
- A `.extra-files` config file with comments explaining how to use it properly.
- A (very simple) `Makefile` to go with the above.

---

#### TODO

- In `download_submissions.py` add an empty file called `late.submit` or something into the student directory for late submissions (and similar for missing), so they aren't ignored by the marker and left with a blank mark. (`automark.sh` would have to look for this file and output a message into their log, and put 0 marks)

- Add the ability to check for the number of warnings in `compile.log` and deduct marks based off some metric that is customizable by the user. For instance, -1 per warning up till a cap of -15 marks. (more important for assignments, not exercises)

- Test if it is faster to download a list of all submission data (for all students) first, instead of doing it one-by-one. Downloading is the slowest part of the automarker.

- More tests to make sure it is robust.

- Have some mechanism to verify if the code didn't exit prematurely with exit status 0. (For now this is avoidable by simply searching for `exit(0)` in the student's code, but it doesn't seem like a feasible long term solution). Maybe modify `templates/tests.c` to make a temporary file after running each test, which the automarker then looks for to make sure the test case actually ended. Downside is that you now have to remember to make this file if you are not using the given template.


